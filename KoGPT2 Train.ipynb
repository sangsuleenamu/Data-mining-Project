{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b16e7346",
   "metadata": {},
   "source": [
    "Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b7ea72b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ballad corpus length: 717967\n",
      "hiphop corpus length: 1525471\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import re\n",
    "\n",
    "# 발라드 크롤링 데이터 전처리\n",
    "path_ballad = './song_folder/lyric_data_ballad.txt'\n",
    "with io.open(path_ballad, encoding='utf-8') as f_ballad: #텍스트 파일을 f에 읽어옴 (한국어 인코딩인 CP949사용 )\n",
    "    text_ballad = f_ballad.read().lower() #f에 내용들을 소문자화\n",
    "\n",
    "text_ballad = re.sub(r'\\n', ' ', text_ballad) #줄바꿈을 공백으로\n",
    "text_ballad = re.sub(r'-', ' ', text_ballad) # '-'를 공백으로\n",
    "text_ballad = re.sub('[^a-zA-Z0-9ㄱ-ㅣ가-힣]',' ',text_ballad) # 한국어와 영어, 숫자 제외 다른 언어 제거\n",
    "text_ballad = ' '.join(text_ballad.split()) #여러개의 공백을 제거하고 하나의 문자열로 만듦\n",
    "\n",
    "# 힙합 크롤링 데이터 전처리\n",
    "path_hiphop = './song_folder/lyric_data_hiphop.txt'\n",
    "with io.open(path_hiphop, encoding='utf-8') as f_hiphop: #텍스트 파일을 f에 읽어옴 (한국어 인코딩인 CP949사용 )\n",
    "    text_hiphop = f_hiphop.read().lower() #f에 내용들을 소문자화\n",
    "\n",
    "text_hiphop = re.sub(r'\\n', ' ', text_hiphop) #줄바꿈을 공백으로\n",
    "text_hiphop = re.sub(r'-', ' ', text_hiphop) # '-'를 공백으로\n",
    "text_hiphop = re.sub('[^a-zA-Z0-9ㄱ-ㅣ가-힣]',' ',text_hiphop) # 한국어와 영어, 숫자 제외 다른 언어 제거\n",
    "text_hiphop = ' '.join(text_hiphop.split()) #여러개의 공백을 제거하고 하나의 문자열로 만듦\n",
    "\n",
    "print('ballad corpus length:', len(text_ballad)) \n",
    "print('hiphop corpus length:', len(text_hiphop)) \n",
    "\n",
    "# 전처리 후 텍스트파일로 추출\n",
    "f1 = open('./song_folder/lyric_data_ballad_preprocessed.txt', 'a', encoding='utf-8')\n",
    "f1.write(text_ballad) \n",
    "f1.close()\n",
    "f2 = open('./song_folder/lyric_data_hiphop_preprocessed.txt', 'a', encoding='utf-8')\n",
    "f2.write(text_hiphop) \n",
    "f2.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c1b2b5",
   "metadata": {},
   "source": [
    "Vectorize Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f32b0c6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
      "The class this function is called from is 'PreTrainedTokenizerFast'.\n",
      "***** Running training *****\n",
      "  Num examples = 2598\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1625\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1625' max='1625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1625/1625 3:09:33, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>3.412900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.532900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.935800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./models_ballad\\checkpoint-500\n",
      "Configuration saved in ./models_ballad\\checkpoint-500\\config.json\n",
      "Model weights saved in ./models_ballad\\checkpoint-500\\pytorch_model.bin\n",
      "Saving model checkpoint to ./models_ballad\\checkpoint-1000\n",
      "Configuration saved in ./models_ballad\\checkpoint-1000\\config.json\n",
      "Model weights saved in ./models_ballad\\checkpoint-1000\\pytorch_model.bin\n",
      "Saving model checkpoint to ./models_ballad\\checkpoint-1500\n",
      "Configuration saved in ./models_ballad\\checkpoint-1500\\config.json\n",
      "Model weights saved in ./models_ballad\\checkpoint-1500\\pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Saving model checkpoint to ./models_ballad\n",
      "Configuration saved in ./models_ballad\\config.json\n",
      "Model weights saved in ./models_ballad\\pytorch_model.bin\n",
      "loading file https://huggingface.co/skt/kogpt2-base-v2/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/skt/kogpt2-base-v2/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/skt/kogpt2-base-v2/resolve/main/tokenizer_config.json from cache at None\n",
      "loading file https://huggingface.co/skt/kogpt2-base-v2/resolve/main/tokenizer.json from cache at C:\\Users\\tkana/.cache\\huggingface\\transformers\\fd8418e6675550cbca8ad6c102d717aa89372eb7a632ad3168300c7fed43491c.db074bfdd88bec54455de5ee2400efdbc64d4acf449a44d5f314e79c1eadc611\n",
      "loading configuration file https://huggingface.co/skt/kogpt2-base-v2/resolve/main/config.json from cache at C:\\Users\\tkana/.cache\\huggingface\\transformers\\13bb826cf24517d7849a701e02452715a67c5e560142be3d4735442b2a545809.6b384eec6effdd44287f67715cd55bd0dff2cf846d843b932b43ba7b632b8b1e\n",
      "Model config GPT2Config {\n",
      "  \"_num_labels\": 1,\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"author\": \"Heewon Jeon(madjakarta@gmail.com)\",\n",
      "  \"bos_token_id\": 0,\n",
      "  \"created_date\": \"2021-04-28\",\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"license\": \"CC-BY-NC-SA 4.0\",\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.12.5\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 51200\n",
      "}\n",
      "\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
      "The class this function is called from is 'PreTrainedTokenizerFast'.\n",
      "C:\\Users\\tkana\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\data\\datasets\\language_modeling.py:54: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/master/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n",
      "Creating features from dataset file at ./song_folder\n",
      "Saving features into cached file ./song_folder\\cached_lm_PreTrainedTokenizerFast_128_lyric_data_hiphop_preprocessed.txt [took 0.017 s]\n",
      "tokenizer config file saved in ./models_hiphop\\tokenizer_config.json\n",
      "Special tokens file saved in ./models_hiphop\\special_tokens_map.json\n",
      "loading configuration file https://huggingface.co/skt/kogpt2-base-v2/resolve/main/config.json from cache at C:\\Users\\tkana/.cache\\huggingface\\transformers\\13bb826cf24517d7849a701e02452715a67c5e560142be3d4735442b2a545809.6b384eec6effdd44287f67715cd55bd0dff2cf846d843b932b43ba7b632b8b1e\n",
      "Model config GPT2Config {\n",
      "  \"_num_labels\": 1,\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"author\": \"Heewon Jeon(madjakarta@gmail.com)\",\n",
      "  \"bos_token_id\": 0,\n",
      "  \"created_date\": \"2021-04-28\",\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"license\": \"CC-BY-NC-SA 4.0\",\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.12.5\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 51200\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/skt/kogpt2-base-v2/resolve/main/pytorch_model.bin from cache at C:\\Users\\tkana/.cache\\huggingface\\transformers\\495b405e3742953dbcc56685d1560fa02a2d86fc50b891868990a4471b06c934.4ebf112d34c2c8fc657866680005d92d21859c52c0ef5e941fa640129b2f8f88\n",
      "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at skt/kogpt2-base-v2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "Configuration saved in ./models_hiphop\\config.json\n",
      "Model weights saved in ./models_hiphop\\pytorch_model.bin\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "***** Running training *****\n",
      "  Num examples = 5779\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3615\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3615' max='3615' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3615/3615 6:55:27, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>3.977100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>3.437300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>3.033900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>2.628700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>2.334000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>2.185400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>1.982800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./models_hiphop\\checkpoint-500\n",
      "Configuration saved in ./models_hiphop\\checkpoint-500\\config.json\n",
      "Model weights saved in ./models_hiphop\\checkpoint-500\\pytorch_model.bin\n",
      "Saving model checkpoint to ./models_hiphop\\checkpoint-1000\n",
      "Configuration saved in ./models_hiphop\\checkpoint-1000\\config.json\n",
      "Model weights saved in ./models_hiphop\\checkpoint-1000\\pytorch_model.bin\n",
      "Saving model checkpoint to ./models_hiphop\\checkpoint-1500\n",
      "Configuration saved in ./models_hiphop\\checkpoint-1500\\config.json\n",
      "Model weights saved in ./models_hiphop\\checkpoint-1500\\pytorch_model.bin\n",
      "Saving model checkpoint to ./models_hiphop\\checkpoint-2000\n",
      "Configuration saved in ./models_hiphop\\checkpoint-2000\\config.json\n",
      "Model weights saved in ./models_hiphop\\checkpoint-2000\\pytorch_model.bin\n",
      "Saving model checkpoint to ./models_hiphop\\checkpoint-2500\n",
      "Configuration saved in ./models_hiphop\\checkpoint-2500\\config.json\n",
      "Model weights saved in ./models_hiphop\\checkpoint-2500\\pytorch_model.bin\n",
      "Saving model checkpoint to ./models_hiphop\\checkpoint-3000\n",
      "Configuration saved in ./models_hiphop\\checkpoint-3000\\config.json\n",
      "Model weights saved in ./models_hiphop\\checkpoint-3000\\pytorch_model.bin\n",
      "Saving model checkpoint to ./models_hiphop\\checkpoint-3500\n",
      "Configuration saved in ./models_hiphop\\checkpoint-3500\\config.json\n",
      "Model weights saved in ./models_hiphop\\checkpoint-3500\\pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Saving model checkpoint to ./models_hiphop\n",
      "Configuration saved in ./models_hiphop\\config.json\n",
      "Model weights saved in ./models_hiphop\\pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "# KOGPT2의 transformers를 불러온다.\n",
    "# transformers는 자연어 처리 모델로, RNN을 사용하지 않고 Attention 만으로도 충분이 seq2seq 자연어 처리를 할 수 있다.\n",
    "\n",
    "from transformers import TextDataset, DataCollatorForLanguageModeling\n",
    "from transformers import GPT2LMHeadModel\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "def load_dataset(file_path, tokenizer, block_size = 128):\n",
    "    dataset = TextDataset(\n",
    "        tokenizer = tokenizer,\n",
    "        file_path = file_path,\n",
    "        block_size = block_size,\n",
    "    )\n",
    "    return dataset\n",
    "\n",
    "def load_data_collator(tokenizer, mlm = False):\n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer = tokenizer,\n",
    "        mlm = mlm,\n",
    "    )\n",
    "    return data_collator\n",
    "\n",
    "def train(train_file_path,model_name,\n",
    "         output_dir,\n",
    "         overwrite_output_dir,\n",
    "         per_device_train_batch_size,\n",
    "         num_train_epochs,\n",
    "         save_steps):\n",
    "    \n",
    "    # Text를 여러개의 Token으로 분류하며 보통 공백, 구두점, 특수문자 등으로 분류한다.\n",
    "    tokenizer = PreTrainedTokenizerFast.from_pretrained(model_name,\n",
    "                bos_token='<s>', eos_token='</s>', unk_token='<unk>',\n",
    "                pad_token='<pad>', mask_token='<mask>')\n",
    "    train_dataset = load_dataset(train_file_path, tokenizer)\n",
    "    data_collator = load_data_collator(tokenizer)\n",
    "    \n",
    "    tokenizer.save_pretrained(output_dir, legacy_format=False)\n",
    "    \n",
    "    model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "    \n",
    "    model.save_pretrained(output_dir)\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "            output_dir=output_dir,\n",
    "            overwrite_output_dir=overwrite_output_dir,\n",
    "            per_device_train_batch_size=per_device_train_batch_size,\n",
    "            num_train_epochs=num_train_epochs,\n",
    "    )\n",
    "    \n",
    "    trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            data_collator=data_collator,\n",
    "            train_dataset=train_dataset,\n",
    "    )\n",
    "    \n",
    "    trainer.train()\n",
    "    trainer.save_model()\n",
    "    \n",
    "model_name = 'skt/kogpt2-base-v2'\n",
    "overwrite_output_dir = False\n",
    "per_device_train_batch_size = 8\n",
    "num_train_epochs = 5.0\n",
    "save_steps = 500\n",
    "\n",
    "# 발라드 모델 학습\n",
    "train(\n",
    "    train_file_path='./song_folder/lyric_data_ballad_preprocessed.txt',\n",
    "    model_name=model_name,\n",
    "    output_dir='./models_ballad',\n",
    "    overwrite_output_dir=overwrite_output_dir,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    save_steps=save_steps\n",
    ")\n",
    "\n",
    "#힙합 모델 학습\n",
    "train(\n",
    "    train_file_path='./song_folder/lyric_data_hiphop_preprocessed.txt',\n",
    "    model_name=model_name,\n",
    "    output_dir='./models_hiphop',\n",
    "    overwrite_output_dir=overwrite_output_dir,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    save_steps=save_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "1d19c72a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "당신은 누구인가요? 산타클로스\n",
      "\n",
      "안녕하세요. 오늘 당신의 하루를 노래로 만들어 드릴게요.\n",
      "\n",
      "산타클로스님의 이야기를 들려주세요 : 메리크리스마스\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ./models_ballad\\config.json\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"skt/kogpt2-base-v2\",\n",
      "  \"_num_labels\": 1,\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"author\": \"Heewon Jeon(madjakarta@gmail.com)\",\n",
      "  \"bos_token_id\": 0,\n",
      "  \"created_date\": \"2021-04-28\",\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"license\": \"CC-BY-NC-SA 4.0\",\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.12.5\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 51200\n",
      "}\n",
      "\n",
      "loading weights file ./models_ballad\\pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================\n",
      "노래를 시작합니다.\n",
      "======================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at ./models_ballad.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "Didn't find file ./models_ballad\\added_tokens.json. We won't load it.\n",
      "loading file None\n",
      "loading file ./models_ballad\\special_tokens_map.json\n",
      "loading file ./models_ballad\\tokenizer_config.json\n",
      "loading file ./models_ballad\\tokenizer.json\n",
      "loading configuration file ./models_hiphop\\config.json\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"skt/kogpt2-base-v2\",\n",
      "  \"_num_labels\": 1,\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"author\": \"Heewon Jeon(madjakarta@gmail.com)\",\n",
      "  \"bos_token_id\": 0,\n",
      "  \"created_date\": \"2021-04-28\",\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"license\": \"CC-BY-NC-SA 4.0\",\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.12.5\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 51200\n",
      "}\n",
      "\n",
      "loading weights file ./models_hiphop\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at ./models_hiphop.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "Didn't find file ./models_hiphop\\added_tokens.json. We won't load it.\n",
      "loading file None\n",
      "loading file ./models_hiphop\\special_tokens_map.json\n",
      "loading file ./models_hiphop\\tokenizer_config.json\n",
      "loading file ./models_hiphop\\tokenizer.json\n",
      "loading configuration file ./models_ballad\\config.json\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"skt/kogpt2-base-v2\",\n",
      "  \"_num_labels\": 1,\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"author\": \"Heewon Jeon(madjakarta@gmail.com)\",\n",
      "  \"bos_token_id\": 0,\n",
      "  \"created_date\": \"2021-04-28\",\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"license\": \"CC-BY-NC-SA 4.0\",\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.12.5\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 51200\n",
      "}\n",
      "\n",
      "loading weights file ./models_ballad\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at ./models_ballad.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "Didn't find file ./models_ballad\\added_tokens.json. We won't load it.\n",
      "loading file None\n",
      "loading file ./models_ballad\\special_tokens_map.json\n",
      "loading file ./models_ballad\\tokenizer_config.json\n",
      "loading file ./models_ballad\\tokenizer.json\n",
      "loading configuration file ./models_hiphop\\config.json\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"skt/kogpt2-base-v2\",\n",
      "  \"_num_labels\": 1,\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"author\": \"Heewon Jeon(madjakarta@gmail.com)\",\n",
      "  \"bos_token_id\": 0,\n",
      "  \"created_date\": \"2021-04-28\",\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"license\": \"CC-BY-NC-SA 4.0\",\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.12.5\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 51200\n",
      "}\n",
      "\n",
      "loading weights file ./models_hiphop\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at ./models_hiphop.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "Didn't find file ./models_hiphop\\added_tokens.json. We won't load it.\n",
      "loading file None\n",
      "loading file ./models_hiphop\\special_tokens_map.json\n",
      "loading file ./models_hiphop\\tokenizer_config.json\n",
      "loading file ./models_hiphop\\tokenizer.json\n",
      "loading configuration file ./models_ballad\\config.json\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"skt/kogpt2-base-v2\",\n",
      "  \"_num_labels\": 1,\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"author\": \"Heewon Jeon(madjakarta@gmail.com)\",\n",
      "  \"bos_token_id\": 0,\n",
      "  \"created_date\": \"2021-04-28\",\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"license\": \"CC-BY-NC-SA 4.0\",\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.12.5\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 51200\n",
      "}\n",
      "\n",
      "loading weights file ./models_ballad\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at ./models_ballad.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "Didn't find file ./models_ballad\\added_tokens.json. We won't load it.\n",
      "loading file None\n",
      "loading file ./models_ballad\\special_tokens_map.json\n",
      "loading file ./models_ballad\\tokenizer_config.json\n",
      "loading file ./models_ballad\\tokenizer.json\n",
      "loading configuration file ./models_hiphop\\config.json\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"skt/kogpt2-base-v2\",\n",
      "  \"_num_labels\": 1,\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"author\": \"Heewon Jeon(madjakarta@gmail.com)\",\n",
      "  \"bos_token_id\": 0,\n",
      "  \"created_date\": \"2021-04-28\",\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"license\": \"CC-BY-NC-SA 4.0\",\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.12.5\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 51200\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading weights file ./models_hiphop\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at ./models_hiphop.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "Didn't find file ./models_hiphop\\added_tokens.json. We won't load it.\n",
      "loading file None\n",
      "loading file ./models_hiphop\\special_tokens_map.json\n",
      "loading file ./models_hiphop\\tokenizer_config.json\n",
      "loading file ./models_hiphop\\tokenizer.json\n",
      "loading configuration file ./models_ballad\\config.json\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"skt/kogpt2-base-v2\",\n",
      "  \"_num_labels\": 1,\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"author\": \"Heewon Jeon(madjakarta@gmail.com)\",\n",
      "  \"bos_token_id\": 0,\n",
      "  \"created_date\": \"2021-04-28\",\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"license\": \"CC-BY-NC-SA 4.0\",\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.12.5\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 51200\n",
      "}\n",
      "\n",
      "loading weights file ./models_ballad\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at ./models_ballad.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "Didn't find file ./models_ballad\\added_tokens.json. We won't load it.\n",
      "loading file None\n",
      "loading file ./models_ballad\\special_tokens_map.json\n",
      "loading file ./models_ballad\\tokenizer_config.json\n",
      "loading file ./models_ballad\\tokenizer.json\n",
      "loading configuration file ./models_hiphop\\config.json\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"skt/kogpt2-base-v2\",\n",
      "  \"_num_labels\": 1,\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"author\": \"Heewon Jeon(madjakarta@gmail.com)\",\n",
      "  \"bos_token_id\": 0,\n",
      "  \"created_date\": \"2021-04-28\",\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"license\": \"CC-BY-NC-SA 4.0\",\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.12.5\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 51200\n",
      "}\n",
      "\n",
      "loading weights file ./models_hiphop\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at ./models_hiphop.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "Didn't find file ./models_hiphop\\added_tokens.json. We won't load it.\n",
      "loading file None\n",
      "loading file ./models_hiphop\\special_tokens_map.json\n",
      "loading file ./models_hiphop\\tokenizer_config.json\n",
      "loading file ./models_hiphop\\tokenizer.json\n",
      "loading configuration file ./models_ballad\\config.json\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"skt/kogpt2-base-v2\",\n",
      "  \"_num_labels\": 1,\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"author\": \"Heewon Jeon(madjakarta@gmail.com)\",\n",
      "  \"bos_token_id\": 0,\n",
      "  \"created_date\": \"2021-04-28\",\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"license\": \"CC-BY-NC-SA 4.0\",\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.12.5\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 51200\n",
      "}\n",
      "\n",
      "loading weights file ./models_ballad\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at ./models_ballad.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "Didn't find file ./models_ballad\\added_tokens.json. We won't load it.\n",
      "loading file None\n",
      "loading file ./models_ballad\\special_tokens_map.json\n",
      "loading file ./models_ballad\\tokenizer_config.json\n",
      "loading file ./models_ballad\\tokenizer.json\n",
      "loading configuration file ./models_hiphop\\config.json\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"skt/kogpt2-base-v2\",\n",
      "  \"_num_labels\": 1,\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"author\": \"Heewon Jeon(madjakarta@gmail.com)\",\n",
      "  \"bos_token_id\": 0,\n",
      "  \"created_date\": \"2021-04-28\",\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"license\": \"CC-BY-NC-SA 4.0\",\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.12.5\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 51200\n",
      "}\n",
      "\n",
      "loading weights file ./models_hiphop\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at ./models_hiphop.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "Didn't find file ./models_hiphop\\added_tokens.json. We won't load it.\n",
      "loading file None\n",
      "loading file ./models_hiphop\\special_tokens_map.json\n",
      "loading file ./models_hiphop\\tokenizer_config.json\n",
      "loading file ./models_hiphop\\tokenizer.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "노래 가사 생성이 완료되었습니다!\n"
     ]
    }
   ],
   "source": [
    "from transformers import PreTrainedTokenizerFast, GPT2LMHeadModel\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(action='ignore') \n",
    "\n",
    "\n",
    "def load_model(model_path):\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "    return model\n",
    "\n",
    "def load_tokenizer(tokenizer_path):\n",
    "    tokenizer = PreTrainedTokenizerFast.from_pretrained(tokenizer_path)\n",
    "    return tokenizer\n",
    "\n",
    "#발라드 텍스트 생성\n",
    "def generate_text_ballad(sequence, max_length):\n",
    "    model_path = './models_ballad'\n",
    "    model = load_model(model_path)\n",
    "    tokenizer = load_tokenizer(model_path)\n",
    "    ids = tokenizer.encode(f'{sequence},', return_tensors='pt')\n",
    "    final_outputs = model.generate(\n",
    "        ids,\n",
    "        do_sample=True,\n",
    "        repetition_penalty=1.1, #반복 제거\n",
    "        #no_repeat_ngram_size=4,\n",
    "        #temperature=100.0,\n",
    "        max_length=max_length,\n",
    "        pad_token_id=model.config.pad_token_id,\n",
    "        top_k=50, #토큰 확률분포에서 확률값이 가장 높은 k개 중에서 선택\n",
    "        top_p=0.95 #확률값이 높은 순서대로 내림차순 정렬을 한 뒤 누적 확률값이  p  이하인 단어들 가운데 선택\n",
    "    )\n",
    "    return tokenizer.decode(final_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "def generate_text_hiphop(sequence, max_length):\n",
    "    model_path = './models_hiphop'\n",
    "    model = load_model(model_path)\n",
    "    tokenizer = load_tokenizer(model_path)\n",
    "    ids = tokenizer.encode(f'{sequence},', return_tensors='pt')\n",
    "    final_outputs = model.generate(\n",
    "        ids,\n",
    "        do_sample=True,\n",
    "        repetition_penalty=1.1, #반복 제거\n",
    "        max_length=max_length,\n",
    "        pad_token_id=model.config.pad_token_id,\n",
    "        top_k=50, #토큰 확률분포에서 확률값이 가장 높은 k개 중에서 선택\n",
    "        top_p=0.95 #확률값이 높은 순서대로 내림차순 정렬을 한 뒤 누적 확률값이  p  이하인 단어들 가운데 선택\n",
    "    )\n",
    "    return tokenizer.decode(final_outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "while(True):\n",
    "    your_name = input(\"당신은 누구인가요? \")\n",
    "    sequence = input(\"\\n안녕하세요. 오늘 당신의 하루를 노래로 만들어 드릴게요.\\n\\n\"+your_name + \"님의 이야기를 들려주세요 : \")\n",
    "\n",
    "    max_len = len(sequence)*3 + 100-len(sequence)*2\n",
    "    print(\"======================================================\\n노래를 시작합니다.\\n======================================================\")\n",
    "    break;\n",
    "      \n",
    "result = [[0 for col in range(2)] for row in range(5)]\n",
    "    \n",
    "for i in range(5):\n",
    "    result[i][0] = generate_text_ballad(sequence,max_len)\n",
    "    result[i][1] = generate_text_hiphop(sequence,max_len)\n",
    "    \n",
    "print(\"\\n노래 가사 생성이 완료되었습니다!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "7f6354e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(result)\n",
    "df.columns = ['발라드', '힙합']\n",
    "pd.set_option('display.colheader_justify','right')\n",
    "pd.set_option('display.max_columns', None) # 모든 열 출력\n",
    "pd.set_option('display.max_colwidth',None) # 열의 표시되는 글자수 제한 해제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "767b0cd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "산타클로스님의 오늘의 이야기를 노래로 만들었어요 ♬\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_7c352_ th {\n",
       "  text-align: center;\n",
       "}\n",
       "#T_7c352_row0_col0, #T_7c352_row0_col1, #T_7c352_row1_col0, #T_7c352_row1_col1, #T_7c352_row2_col0, #T_7c352_row2_col1, #T_7c352_row3_col0, #T_7c352_row3_col1, #T_7c352_row4_col0, #T_7c352_row4_col1 {\n",
       "  text-align: left;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_7c352_\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th class=\"col_heading level0 col0\" >발라드</th>\n",
       "      <th class=\"col_heading level0 col1\" >힙합</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_7c352_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_7c352_row0_col0\" class=\"data row0 col0\" >메리크리스마스, 화이트 크리스마스 트리 주고 싶어 긴긴 밤새도록 천사 같은 그대와 눈 맞추며 넌 여기까지 너만 있길 마음 다해 기도해 내 맘속 한 사람 네가 바로 너라서 그래서 나뿐이야 라 라 라 라라라 랄라 라 라라라랄라 난 네 안에 있어 라라라랄라 너를 기다렸어 너무 오랜 밤 새벽을 깨우는 작은 소나기는 언제나 너를 떠올리고 있었지 날 감싸 준 너야 사랑이었단걸 말</td>\n",
       "      <td id=\"T_7c352_row0_col1\" class=\"data row0 col1\" >메리크리스마스, 크리스마스 캐롤 더 불러봅니다 같이 제발 call me nowhere so good lifestyle to be times i don t want your love and let me out for a foolish girl i wanna be with u baby some thing 막쓰고 다쓰고 쓴거를 다 pay back 이렇게라도 해야 했나 난 도망가지 않았으면 해 우리 잡힘</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7c352_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_7c352_row1_col0\" class=\"data row1 col0\" >메리크리스마스, 내맘속 한사람 노랠 듣다 울먹이며 달려와 난 널 기다려 다시 한 번 너를 바라본다 오랜 기다림이 지나가고 익숙한 냄새 가득한 이 거리에 아직 낯선 이의 웃음들이 가득해 그 어떤 고민도 힘겨워하지 않아 지금 이 순간만 기다려줘 i love you 오늘도 난 너의 품 안에서 잠들 수 있을까 기쁜 마음으로 i love you 네게 속삭여줄게 멀리 있어도 느낄 수 있는 나만의 손길 i love y</td>\n",
       "      <td id=\"T_7c352_row1_col1\" class=\"data row1 col1\" >메리크리스마스, i m up on a rockstar 바코드같이 규칙적인 cold like a lightlightlightlightlightlightlightlightlightrunkrr 날 보며 활짝 웃어 계속 반복해 널 기다려주는 친구야 너에게 미안 너무 늦어버린 것 같아 너무 많은 시간들이 있지만 난 항상 내 옆을 지나쳐왔으니까 조금만 참다참다참다참다 반복하다끝난 건가 어젯밤 내가 본</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7c352_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_7c352_row2_col0\" class=\"data row2 col0\" >메리크리스마스, 내 맘속 모두 stop love you 너에게 고백할래 stop love you 너와 꿈꿨던 시간 언제나 난 영원할 거야 the villain it s a way holding is not starry 기나긴 겨울밤은 유난히 포근한 바람이구나 널 만나면 늘 네게만 항상 웃곤 해 이제 조금 지나면 나아질까 생각했는데 예상을 깨고 네게 입 맞춘 순간 나를 향해 와 크게 들리네 하얀 눈</td>\n",
       "      <td id=\"T_7c352_row2_col1\" class=\"data row2 col1\" >메리크리스마스, sorry 모두 이겨내고 bitech는 더 커져 주머니에 iphone을 채워 coway에 어질러진 수돗물에 버틴 물 drinking motto에 담지 hey you everyday get it we re here with you here without you i ll change this time out pretty lie everything everyday get it we</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7c352_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_7c352_row3_col0\" class=\"data row3 col0\" >메리크리스마스, 슬프지만 행복한 이 순간 함께 하니까 꿈꿔온 그 모습 여전히 예쁘다 내 손을 잡고 걷는 이 christmas 꿈만 같아 전부 다 내 세상 같아 나에게는 이 세상 무엇보다 소중한 것 같아 넌 한 때 꿈에서도 사랑으로 켰던 그 기억만 아직도 선명해 나는 행복했었던 그때로 다시 돌아가 너의 모든 추억이 담긴 이 길을 따라 걸으며 하나 둘 겨울 지나 봄이 오듯이 나도 꽃이 피겠지 당신을 닮아갈래 더 따뜻할게요</td>\n",
       "      <td id=\"T_7c352_row3_col1\" class=\"data row3 col1\" >메리크리스마스, maybe is your love cuz i know but i won t never let s get our fam 내 친구들은 곧 없어져 버릴 shit you gotta high 나는 아직도 slave i can do that my life 오늘 밤은 전부 다 riskin pain 오늘은 쉬워 we re just go into you yeah i am s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7c352_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_7c352_row4_col0\" class=\"data row4 col0\" >메리크리스마스, 크리스마스 선물하나 준비못했지만 하얗게 눈이 내리던 그날 생각하며 새하얘진 얼굴 하얀 눈꽃이 날리는 이 순간이 올것만 같아 나 오늘도 하얗게 눈이 내리던 그날 생각해 떠올리며 사랑했었던 그날들 생각하며 잠 못 드는 나의 마음을 안쓰럽단말야 안녕 안녕아 나 오늘도 하얗게 눈이 내리던 그날 생각하면 떠올리며 사랑했었던 그 시간들 생각하며 이제서야 내 마음 차갑게 식어가는 나를 보며 한없는 미소가 번</td>\n",
       "      <td id=\"T_7c352_row4_col1\" class=\"data row4 col1\" >메리크리스마스, feel myself and pause out of the queeneration apart now young bass my time but we made for me like i know that booth i can never get surfficing for the next chance where can never let s play with me done had to be t</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x2b0cc8dcfa0>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(your_name+\"님의 오늘의 이야기를 노래로 만들었어요 ♬\")\n",
    "dfStyler = df.style.set_properties(**{'text-align': 'left'}) # 텍스트 왼쪽 정렬\n",
    "dfStyler.set_table_styles([dict(selector='th', props=[('text-align', 'center')])]) # label 제목 가운데 정렬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9235af5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
