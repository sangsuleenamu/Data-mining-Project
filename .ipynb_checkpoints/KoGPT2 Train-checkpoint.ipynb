{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b16e7346",
   "metadata": {},
   "source": [
    "Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b7ea72b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ballad corpus length: 717967\n",
      "hiphop corpus length: 1525471\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import re\n",
    "\n",
    "# Î∞úÎùºÎìú ÌÅ¨Î°§ÎßÅ Îç∞Ïù¥ÌÑ∞ Ï†ÑÏ≤òÎ¶¨\n",
    "path_ballad = './song_folder/lyric_data_ballad.txt'\n",
    "with io.open(path_ballad, encoding='utf-8') as f_ballad: #ÌÖçÏä§Ìä∏ ÌååÏùºÏùÑ fÏóê ÏùΩÏñ¥Ïò¥ (ÌïúÍµ≠Ïñ¥ Ïù∏ÏΩîÎî©Ïù∏ CP949ÏÇ¨Ïö© )\n",
    "    text_ballad = f_ballad.read().lower() #fÏóê ÎÇ¥Ïö©Îì§ÏùÑ ÏÜåÎ¨∏ÏûêÌôî\n",
    "\n",
    "text_ballad = re.sub(r'\\n', ' ', text_ballad) #Ï§ÑÎ∞îÍøàÏùÑ Í≥µÎ∞±ÏúºÎ°ú\n",
    "text_ballad = re.sub(r'-', ' ', text_ballad) # '-'Î•º Í≥µÎ∞±ÏúºÎ°ú\n",
    "text_ballad = re.sub('[^a-zA-Z0-9„Ñ±-„Ö£Í∞Ä-Ìû£]',' ',text_ballad) # ÌïúÍµ≠Ïñ¥ÏôÄ ÏòÅÏñ¥, Ïà´Ïûê Ï†úÏô∏ Îã§Î•∏ Ïñ∏Ïñ¥ Ï†úÍ±∞\n",
    "text_ballad = ' '.join(text_ballad.split()) #Ïó¨Îü¨Í∞úÏùò Í≥µÎ∞±ÏùÑ Ï†úÍ±∞ÌïòÍ≥† ÌïòÎÇòÏùò Î¨∏ÏûêÏó¥Î°ú ÎßåÎì¶\n",
    "\n",
    "# ÌûôÌï© ÌÅ¨Î°§ÎßÅ Îç∞Ïù¥ÌÑ∞ Ï†ÑÏ≤òÎ¶¨\n",
    "path_hiphop = './song_folder/lyric_data_hiphop.txt'\n",
    "with io.open(path_hiphop, encoding='utf-8') as f_hiphop: #ÌÖçÏä§Ìä∏ ÌååÏùºÏùÑ fÏóê ÏùΩÏñ¥Ïò¥ (ÌïúÍµ≠Ïñ¥ Ïù∏ÏΩîÎî©Ïù∏ CP949ÏÇ¨Ïö© )\n",
    "    text_hiphop = f_hiphop.read().lower() #fÏóê ÎÇ¥Ïö©Îì§ÏùÑ ÏÜåÎ¨∏ÏûêÌôî\n",
    "\n",
    "text_hiphop = re.sub(r'\\n', ' ', text_hiphop) #Ï§ÑÎ∞îÍøàÏùÑ Í≥µÎ∞±ÏúºÎ°ú\n",
    "text_hiphop = re.sub(r'-', ' ', text_hiphop) # '-'Î•º Í≥µÎ∞±ÏúºÎ°ú\n",
    "text_hiphop = re.sub('[^a-zA-Z0-9„Ñ±-„Ö£Í∞Ä-Ìû£]',' ',text_hiphop) # ÌïúÍµ≠Ïñ¥ÏôÄ ÏòÅÏñ¥, Ïà´Ïûê Ï†úÏô∏ Îã§Î•∏ Ïñ∏Ïñ¥ Ï†úÍ±∞\n",
    "text_hiphop = ' '.join(text_hiphop.split()) #Ïó¨Îü¨Í∞úÏùò Í≥µÎ∞±ÏùÑ Ï†úÍ±∞ÌïòÍ≥† ÌïòÎÇòÏùò Î¨∏ÏûêÏó¥Î°ú ÎßåÎì¶\n",
    "\n",
    "print('ballad corpus length:', len(text_ballad)) \n",
    "print('hiphop corpus length:', len(text_hiphop)) \n",
    "\n",
    "# Ï†ÑÏ≤òÎ¶¨ ÌõÑ ÌÖçÏä§Ìä∏ÌååÏùºÎ°ú Ï∂îÏ∂ú\n",
    "f1 = open('./song_folder/lyric_data_ballad_preprocessed.txt', 'a', encoding='utf-8')\n",
    "f1.write(text_ballad) \n",
    "f1.close()\n",
    "f2 = open('./song_folder/lyric_data_hiphop_preprocessed.txt', 'a', encoding='utf-8')\n",
    "f2.write(text_hiphop) \n",
    "f2.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c1b2b5",
   "metadata": {},
   "source": [
    "Vectorize Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f32b0c6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
      "The class this function is called from is 'PreTrainedTokenizerFast'.\n",
      "***** Running training *****\n",
      "  Num examples = 2598\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1625\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1625' max='1625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1625/1625 3:09:33, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>3.412900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.532900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.935800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./models_ballad\\checkpoint-500\n",
      "Configuration saved in ./models_ballad\\checkpoint-500\\config.json\n",
      "Model weights saved in ./models_ballad\\checkpoint-500\\pytorch_model.bin\n",
      "Saving model checkpoint to ./models_ballad\\checkpoint-1000\n",
      "Configuration saved in ./models_ballad\\checkpoint-1000\\config.json\n",
      "Model weights saved in ./models_ballad\\checkpoint-1000\\pytorch_model.bin\n",
      "Saving model checkpoint to ./models_ballad\\checkpoint-1500\n",
      "Configuration saved in ./models_ballad\\checkpoint-1500\\config.json\n",
      "Model weights saved in ./models_ballad\\checkpoint-1500\\pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Saving model checkpoint to ./models_ballad\n",
      "Configuration saved in ./models_ballad\\config.json\n",
      "Model weights saved in ./models_ballad\\pytorch_model.bin\n",
      "loading file https://huggingface.co/skt/kogpt2-base-v2/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/skt/kogpt2-base-v2/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/skt/kogpt2-base-v2/resolve/main/tokenizer_config.json from cache at None\n",
      "loading file https://huggingface.co/skt/kogpt2-base-v2/resolve/main/tokenizer.json from cache at C:\\Users\\tkana/.cache\\huggingface\\transformers\\fd8418e6675550cbca8ad6c102d717aa89372eb7a632ad3168300c7fed43491c.db074bfdd88bec54455de5ee2400efdbc64d4acf449a44d5f314e79c1eadc611\n",
      "loading configuration file https://huggingface.co/skt/kogpt2-base-v2/resolve/main/config.json from cache at C:\\Users\\tkana/.cache\\huggingface\\transformers\\13bb826cf24517d7849a701e02452715a67c5e560142be3d4735442b2a545809.6b384eec6effdd44287f67715cd55bd0dff2cf846d843b932b43ba7b632b8b1e\n",
      "Model config GPT2Config {\n",
      "  \"_num_labels\": 1,\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"author\": \"Heewon Jeon(madjakarta@gmail.com)\",\n",
      "  \"bos_token_id\": 0,\n",
      "  \"created_date\": \"2021-04-28\",\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"license\": \"CC-BY-NC-SA 4.0\",\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.12.5\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 51200\n",
      "}\n",
      "\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
      "The class this function is called from is 'PreTrainedTokenizerFast'.\n",
      "C:\\Users\\tkana\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\data\\datasets\\language_modeling.py:54: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ü§ó Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/master/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n",
      "Creating features from dataset file at ./song_folder\n",
      "Saving features into cached file ./song_folder\\cached_lm_PreTrainedTokenizerFast_128_lyric_data_hiphop_preprocessed.txt [took 0.017 s]\n",
      "tokenizer config file saved in ./models_hiphop\\tokenizer_config.json\n",
      "Special tokens file saved in ./models_hiphop\\special_tokens_map.json\n",
      "loading configuration file https://huggingface.co/skt/kogpt2-base-v2/resolve/main/config.json from cache at C:\\Users\\tkana/.cache\\huggingface\\transformers\\13bb826cf24517d7849a701e02452715a67c5e560142be3d4735442b2a545809.6b384eec6effdd44287f67715cd55bd0dff2cf846d843b932b43ba7b632b8b1e\n",
      "Model config GPT2Config {\n",
      "  \"_num_labels\": 1,\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"author\": \"Heewon Jeon(madjakarta@gmail.com)\",\n",
      "  \"bos_token_id\": 0,\n",
      "  \"created_date\": \"2021-04-28\",\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"license\": \"CC-BY-NC-SA 4.0\",\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.12.5\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 51200\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/skt/kogpt2-base-v2/resolve/main/pytorch_model.bin from cache at C:\\Users\\tkana/.cache\\huggingface\\transformers\\495b405e3742953dbcc56685d1560fa02a2d86fc50b891868990a4471b06c934.4ebf112d34c2c8fc657866680005d92d21859c52c0ef5e941fa640129b2f8f88\n",
      "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at skt/kogpt2-base-v2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "Configuration saved in ./models_hiphop\\config.json\n",
      "Model weights saved in ./models_hiphop\\pytorch_model.bin\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "***** Running training *****\n",
      "  Num examples = 5779\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3615\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3615' max='3615' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3615/3615 6:55:27, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>3.977100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>3.437300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>3.033900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>2.628700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>2.334000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>2.185400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>1.982800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./models_hiphop\\checkpoint-500\n",
      "Configuration saved in ./models_hiphop\\checkpoint-500\\config.json\n",
      "Model weights saved in ./models_hiphop\\checkpoint-500\\pytorch_model.bin\n",
      "Saving model checkpoint to ./models_hiphop\\checkpoint-1000\n",
      "Configuration saved in ./models_hiphop\\checkpoint-1000\\config.json\n",
      "Model weights saved in ./models_hiphop\\checkpoint-1000\\pytorch_model.bin\n",
      "Saving model checkpoint to ./models_hiphop\\checkpoint-1500\n",
      "Configuration saved in ./models_hiphop\\checkpoint-1500\\config.json\n",
      "Model weights saved in ./models_hiphop\\checkpoint-1500\\pytorch_model.bin\n",
      "Saving model checkpoint to ./models_hiphop\\checkpoint-2000\n",
      "Configuration saved in ./models_hiphop\\checkpoint-2000\\config.json\n",
      "Model weights saved in ./models_hiphop\\checkpoint-2000\\pytorch_model.bin\n",
      "Saving model checkpoint to ./models_hiphop\\checkpoint-2500\n",
      "Configuration saved in ./models_hiphop\\checkpoint-2500\\config.json\n",
      "Model weights saved in ./models_hiphop\\checkpoint-2500\\pytorch_model.bin\n",
      "Saving model checkpoint to ./models_hiphop\\checkpoint-3000\n",
      "Configuration saved in ./models_hiphop\\checkpoint-3000\\config.json\n",
      "Model weights saved in ./models_hiphop\\checkpoint-3000\\pytorch_model.bin\n",
      "Saving model checkpoint to ./models_hiphop\\checkpoint-3500\n",
      "Configuration saved in ./models_hiphop\\checkpoint-3500\\config.json\n",
      "Model weights saved in ./models_hiphop\\checkpoint-3500\\pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Saving model checkpoint to ./models_hiphop\n",
      "Configuration saved in ./models_hiphop\\config.json\n",
      "Model weights saved in ./models_hiphop\\pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "# KOGPT2Ïùò transformersÎ•º Î∂àÎü¨Ïò®Îã§.\n",
    "# transformersÎäî ÏûêÏó∞Ïñ¥ Ï≤òÎ¶¨ Î™®Îç∏Î°ú, RNNÏùÑ ÏÇ¨Ïö©ÌïòÏßÄ ÏïäÍ≥† Attention ÎßåÏúºÎ°úÎèÑ Ï∂©Î∂ÑÏù¥ seq2seq ÏûêÏó∞Ïñ¥ Ï≤òÎ¶¨Î•º Ìï† Ïàò ÏûàÎã§.\n",
    "\n",
    "from transformers import TextDataset, DataCollatorForLanguageModeling\n",
    "from transformers import GPT2LMHeadModel\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "def load_dataset(file_path, tokenizer, block_size = 128):\n",
    "    dataset = TextDataset(\n",
    "        tokenizer = tokenizer,\n",
    "        file_path = file_path,\n",
    "        block_size = block_size,\n",
    "    )\n",
    "    return dataset\n",
    "\n",
    "def load_data_collator(tokenizer, mlm = False):\n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer = tokenizer,\n",
    "        mlm = mlm,\n",
    "    )\n",
    "    return data_collator\n",
    "\n",
    "def train(train_file_path,model_name,\n",
    "         output_dir,\n",
    "         overwrite_output_dir,\n",
    "         per_device_train_batch_size,\n",
    "         num_train_epochs,\n",
    "         save_steps):\n",
    "    \n",
    "    # TextÎ•º Ïó¨Îü¨Í∞úÏùò TokenÏúºÎ°ú Î∂ÑÎ•òÌïòÎ©∞ Î≥¥ÌÜµ Í≥µÎ∞±, Íµ¨ÎëêÏ†ê, ÌäπÏàòÎ¨∏Ïûê Îì±ÏúºÎ°ú Î∂ÑÎ•òÌïúÎã§.\n",
    "    tokenizer = PreTrainedTokenizerFast.from_pretrained(model_name,\n",
    "                bos_token='<s>', eos_token='</s>', unk_token='<unk>',\n",
    "                pad_token='<pad>', mask_token='<mask>')\n",
    "    train_dataset = load_dataset(train_file_path, tokenizer)\n",
    "    data_collator = load_data_collator(tokenizer)\n",
    "    \n",
    "    tokenizer.save_pretrained(output_dir, legacy_format=False)\n",
    "    \n",
    "    model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "    \n",
    "    model.save_pretrained(output_dir)\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "            output_dir=output_dir,\n",
    "            overwrite_output_dir=overwrite_output_dir,\n",
    "            per_device_train_batch_size=per_device_train_batch_size,\n",
    "            num_train_epochs=num_train_epochs,\n",
    "    )\n",
    "    \n",
    "    trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            data_collator=data_collator,\n",
    "            train_dataset=train_dataset,\n",
    "    )\n",
    "    \n",
    "    trainer.train()\n",
    "    trainer.save_model()\n",
    "    \n",
    "model_name = 'skt/kogpt2-base-v2'\n",
    "overwrite_output_dir = False\n",
    "per_device_train_batch_size = 8\n",
    "num_train_epochs = 5.0\n",
    "save_steps = 500\n",
    "\n",
    "# Î∞úÎùºÎìú Î™®Îç∏ ÌïôÏäµ\n",
    "train(\n",
    "    train_file_path='./song_folder/lyric_data_ballad_preprocessed.txt',\n",
    "    model_name=model_name,\n",
    "    output_dir='./models_ballad',\n",
    "    overwrite_output_dir=overwrite_output_dir,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    save_steps=save_steps\n",
    ")\n",
    "\n",
    "#ÌûôÌï© Î™®Îç∏ ÌïôÏäµ\n",
    "train(\n",
    "    train_file_path='./song_folder/lyric_data_hiphop_preprocessed.txt',\n",
    "    model_name=model_name,\n",
    "    output_dir='./models_hiphop',\n",
    "    overwrite_output_dir=overwrite_output_dir,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    save_steps=save_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "1d19c72a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ÎãπÏã†ÏùÄ ÎàÑÍµ¨Ïù∏Í∞ÄÏöî? ÏÇ∞ÌÉÄÌÅ¥Î°úÏä§\n",
      "\n",
      "ÏïàÎÖïÌïòÏÑ∏Ïöî. Ïò§Îäò ÎãπÏã†Ïùò ÌïòÎ£®Î•º ÎÖ∏ÎûòÎ°ú ÎßåÎì§Ïñ¥ ÎìúÎ¶¥Í≤åÏöî.\n",
      "\n",
      "ÏÇ∞ÌÉÄÌÅ¥Î°úÏä§ÎãòÏùò Ïù¥ÏïºÍ∏∞Î•º Îì§Î†§Ï£ºÏÑ∏Ïöî : Î©îÎ¶¨ÌÅ¨Î¶¨Ïä§ÎßàÏä§\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ./models_ballad\\config.json\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"skt/kogpt2-base-v2\",\n",
      "  \"_num_labels\": 1,\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"author\": \"Heewon Jeon(madjakarta@gmail.com)\",\n",
      "  \"bos_token_id\": 0,\n",
      "  \"created_date\": \"2021-04-28\",\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"license\": \"CC-BY-NC-SA 4.0\",\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.12.5\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 51200\n",
      "}\n",
      "\n",
      "loading weights file ./models_ballad\\pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================\n",
      "ÎÖ∏ÎûòÎ•º ÏãúÏûëÌï©ÎãàÎã§.\n",
      "======================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at ./models_ballad.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "Didn't find file ./models_ballad\\added_tokens.json. We won't load it.\n",
      "loading file None\n",
      "loading file ./models_ballad\\special_tokens_map.json\n",
      "loading file ./models_ballad\\tokenizer_config.json\n",
      "loading file ./models_ballad\\tokenizer.json\n",
      "loading configuration file ./models_hiphop\\config.json\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"skt/kogpt2-base-v2\",\n",
      "  \"_num_labels\": 1,\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"author\": \"Heewon Jeon(madjakarta@gmail.com)\",\n",
      "  \"bos_token_id\": 0,\n",
      "  \"created_date\": \"2021-04-28\",\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"license\": \"CC-BY-NC-SA 4.0\",\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.12.5\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 51200\n",
      "}\n",
      "\n",
      "loading weights file ./models_hiphop\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at ./models_hiphop.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "Didn't find file ./models_hiphop\\added_tokens.json. We won't load it.\n",
      "loading file None\n",
      "loading file ./models_hiphop\\special_tokens_map.json\n",
      "loading file ./models_hiphop\\tokenizer_config.json\n",
      "loading file ./models_hiphop\\tokenizer.json\n",
      "loading configuration file ./models_ballad\\config.json\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"skt/kogpt2-base-v2\",\n",
      "  \"_num_labels\": 1,\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"author\": \"Heewon Jeon(madjakarta@gmail.com)\",\n",
      "  \"bos_token_id\": 0,\n",
      "  \"created_date\": \"2021-04-28\",\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"license\": \"CC-BY-NC-SA 4.0\",\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.12.5\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 51200\n",
      "}\n",
      "\n",
      "loading weights file ./models_ballad\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at ./models_ballad.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "Didn't find file ./models_ballad\\added_tokens.json. We won't load it.\n",
      "loading file None\n",
      "loading file ./models_ballad\\special_tokens_map.json\n",
      "loading file ./models_ballad\\tokenizer_config.json\n",
      "loading file ./models_ballad\\tokenizer.json\n",
      "loading configuration file ./models_hiphop\\config.json\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"skt/kogpt2-base-v2\",\n",
      "  \"_num_labels\": 1,\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"author\": \"Heewon Jeon(madjakarta@gmail.com)\",\n",
      "  \"bos_token_id\": 0,\n",
      "  \"created_date\": \"2021-04-28\",\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"license\": \"CC-BY-NC-SA 4.0\",\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.12.5\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 51200\n",
      "}\n",
      "\n",
      "loading weights file ./models_hiphop\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at ./models_hiphop.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "Didn't find file ./models_hiphop\\added_tokens.json. We won't load it.\n",
      "loading file None\n",
      "loading file ./models_hiphop\\special_tokens_map.json\n",
      "loading file ./models_hiphop\\tokenizer_config.json\n",
      "loading file ./models_hiphop\\tokenizer.json\n",
      "loading configuration file ./models_ballad\\config.json\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"skt/kogpt2-base-v2\",\n",
      "  \"_num_labels\": 1,\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"author\": \"Heewon Jeon(madjakarta@gmail.com)\",\n",
      "  \"bos_token_id\": 0,\n",
      "  \"created_date\": \"2021-04-28\",\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"license\": \"CC-BY-NC-SA 4.0\",\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.12.5\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 51200\n",
      "}\n",
      "\n",
      "loading weights file ./models_ballad\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at ./models_ballad.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "Didn't find file ./models_ballad\\added_tokens.json. We won't load it.\n",
      "loading file None\n",
      "loading file ./models_ballad\\special_tokens_map.json\n",
      "loading file ./models_ballad\\tokenizer_config.json\n",
      "loading file ./models_ballad\\tokenizer.json\n",
      "loading configuration file ./models_hiphop\\config.json\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"skt/kogpt2-base-v2\",\n",
      "  \"_num_labels\": 1,\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"author\": \"Heewon Jeon(madjakarta@gmail.com)\",\n",
      "  \"bos_token_id\": 0,\n",
      "  \"created_date\": \"2021-04-28\",\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"license\": \"CC-BY-NC-SA 4.0\",\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.12.5\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 51200\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading weights file ./models_hiphop\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at ./models_hiphop.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "Didn't find file ./models_hiphop\\added_tokens.json. We won't load it.\n",
      "loading file None\n",
      "loading file ./models_hiphop\\special_tokens_map.json\n",
      "loading file ./models_hiphop\\tokenizer_config.json\n",
      "loading file ./models_hiphop\\tokenizer.json\n",
      "loading configuration file ./models_ballad\\config.json\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"skt/kogpt2-base-v2\",\n",
      "  \"_num_labels\": 1,\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"author\": \"Heewon Jeon(madjakarta@gmail.com)\",\n",
      "  \"bos_token_id\": 0,\n",
      "  \"created_date\": \"2021-04-28\",\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"license\": \"CC-BY-NC-SA 4.0\",\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.12.5\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 51200\n",
      "}\n",
      "\n",
      "loading weights file ./models_ballad\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at ./models_ballad.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "Didn't find file ./models_ballad\\added_tokens.json. We won't load it.\n",
      "loading file None\n",
      "loading file ./models_ballad\\special_tokens_map.json\n",
      "loading file ./models_ballad\\tokenizer_config.json\n",
      "loading file ./models_ballad\\tokenizer.json\n",
      "loading configuration file ./models_hiphop\\config.json\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"skt/kogpt2-base-v2\",\n",
      "  \"_num_labels\": 1,\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"author\": \"Heewon Jeon(madjakarta@gmail.com)\",\n",
      "  \"bos_token_id\": 0,\n",
      "  \"created_date\": \"2021-04-28\",\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"license\": \"CC-BY-NC-SA 4.0\",\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.12.5\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 51200\n",
      "}\n",
      "\n",
      "loading weights file ./models_hiphop\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at ./models_hiphop.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "Didn't find file ./models_hiphop\\added_tokens.json. We won't load it.\n",
      "loading file None\n",
      "loading file ./models_hiphop\\special_tokens_map.json\n",
      "loading file ./models_hiphop\\tokenizer_config.json\n",
      "loading file ./models_hiphop\\tokenizer.json\n",
      "loading configuration file ./models_ballad\\config.json\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"skt/kogpt2-base-v2\",\n",
      "  \"_num_labels\": 1,\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"author\": \"Heewon Jeon(madjakarta@gmail.com)\",\n",
      "  \"bos_token_id\": 0,\n",
      "  \"created_date\": \"2021-04-28\",\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"license\": \"CC-BY-NC-SA 4.0\",\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.12.5\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 51200\n",
      "}\n",
      "\n",
      "loading weights file ./models_ballad\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at ./models_ballad.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "Didn't find file ./models_ballad\\added_tokens.json. We won't load it.\n",
      "loading file None\n",
      "loading file ./models_ballad\\special_tokens_map.json\n",
      "loading file ./models_ballad\\tokenizer_config.json\n",
      "loading file ./models_ballad\\tokenizer.json\n",
      "loading configuration file ./models_hiphop\\config.json\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"skt/kogpt2-base-v2\",\n",
      "  \"_num_labels\": 1,\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"author\": \"Heewon Jeon(madjakarta@gmail.com)\",\n",
      "  \"bos_token_id\": 0,\n",
      "  \"created_date\": \"2021-04-28\",\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"license\": \"CC-BY-NC-SA 4.0\",\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.12.5\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 51200\n",
      "}\n",
      "\n",
      "loading weights file ./models_hiphop\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at ./models_hiphop.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "Didn't find file ./models_hiphop\\added_tokens.json. We won't load it.\n",
      "loading file None\n",
      "loading file ./models_hiphop\\special_tokens_map.json\n",
      "loading file ./models_hiphop\\tokenizer_config.json\n",
      "loading file ./models_hiphop\\tokenizer.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ÎÖ∏Îûò Í∞ÄÏÇ¨ ÏÉùÏÑ±Ïù¥ ÏôÑÎ£åÎêòÏóàÏäµÎãàÎã§!\n"
     ]
    }
   ],
   "source": [
    "from transformers import PreTrainedTokenizerFast, GPT2LMHeadModel\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(action='ignore') \n",
    "\n",
    "\n",
    "def load_model(model_path):\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "    return model\n",
    "\n",
    "def load_tokenizer(tokenizer_path):\n",
    "    tokenizer = PreTrainedTokenizerFast.from_pretrained(tokenizer_path)\n",
    "    return tokenizer\n",
    "\n",
    "#Î∞úÎùºÎìú ÌÖçÏä§Ìä∏ ÏÉùÏÑ±\n",
    "def generate_text_ballad(sequence, max_length):\n",
    "    model_path = './models_ballad'\n",
    "    model = load_model(model_path)\n",
    "    tokenizer = load_tokenizer(model_path)\n",
    "    ids = tokenizer.encode(f'{sequence},', return_tensors='pt')\n",
    "    final_outputs = model.generate(\n",
    "        ids,\n",
    "        do_sample=True,\n",
    "        repetition_penalty=1.1, #Î∞òÎ≥µ Ï†úÍ±∞\n",
    "        #no_repeat_ngram_size=4,\n",
    "        #temperature=100.0,\n",
    "        max_length=max_length,\n",
    "        pad_token_id=model.config.pad_token_id,\n",
    "        top_k=50, #ÌÜ†ÌÅ∞ ÌôïÎ•†Î∂ÑÌè¨ÏóêÏÑú ÌôïÎ•†Í∞íÏù¥ Í∞ÄÏû• ÎÜíÏùÄ kÍ∞ú Ï§ëÏóêÏÑú ÏÑ†ÌÉù\n",
    "        top_p=0.95 #ÌôïÎ•†Í∞íÏù¥ ÎÜíÏùÄ ÏàúÏÑúÎåÄÎ°ú ÎÇ¥Î¶ºÏ∞®Ïàú Ï†ïÎ†¨ÏùÑ Ìïú Îí§ ÎàÑÏ†Å ÌôïÎ•†Í∞íÏù¥  p  Ïù¥ÌïòÏù∏ Îã®Ïñ¥Îì§ Í∞ÄÏö¥Îç∞ ÏÑ†ÌÉù\n",
    "    )\n",
    "    return tokenizer.decode(final_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "def generate_text_hiphop(sequence, max_length):\n",
    "    model_path = './models_hiphop'\n",
    "    model = load_model(model_path)\n",
    "    tokenizer = load_tokenizer(model_path)\n",
    "    ids = tokenizer.encode(f'{sequence},', return_tensors='pt')\n",
    "    final_outputs = model.generate(\n",
    "        ids,\n",
    "        do_sample=True,\n",
    "        repetition_penalty=1.1, #Î∞òÎ≥µ Ï†úÍ±∞\n",
    "        max_length=max_length,\n",
    "        pad_token_id=model.config.pad_token_id,\n",
    "        top_k=50, #ÌÜ†ÌÅ∞ ÌôïÎ•†Î∂ÑÌè¨ÏóêÏÑú ÌôïÎ•†Í∞íÏù¥ Í∞ÄÏû• ÎÜíÏùÄ kÍ∞ú Ï§ëÏóêÏÑú ÏÑ†ÌÉù\n",
    "        top_p=0.95 #ÌôïÎ•†Í∞íÏù¥ ÎÜíÏùÄ ÏàúÏÑúÎåÄÎ°ú ÎÇ¥Î¶ºÏ∞®Ïàú Ï†ïÎ†¨ÏùÑ Ìïú Îí§ ÎàÑÏ†Å ÌôïÎ•†Í∞íÏù¥  p  Ïù¥ÌïòÏù∏ Îã®Ïñ¥Îì§ Í∞ÄÏö¥Îç∞ ÏÑ†ÌÉù\n",
    "    )\n",
    "    return tokenizer.decode(final_outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "while(True):\n",
    "    your_name = input(\"ÎãπÏã†ÏùÄ ÎàÑÍµ¨Ïù∏Í∞ÄÏöî? \")\n",
    "    sequence = input(\"\\nÏïàÎÖïÌïòÏÑ∏Ïöî. Ïò§Îäò ÎãπÏã†Ïùò ÌïòÎ£®Î•º ÎÖ∏ÎûòÎ°ú ÎßåÎì§Ïñ¥ ÎìúÎ¶¥Í≤åÏöî.\\n\\n\"+your_name + \"ÎãòÏùò Ïù¥ÏïºÍ∏∞Î•º Îì§Î†§Ï£ºÏÑ∏Ïöî : \")\n",
    "\n",
    "    max_len = len(sequence)*3 + 100-len(sequence)*2\n",
    "    print(\"======================================================\\nÎÖ∏ÎûòÎ•º ÏãúÏûëÌï©ÎãàÎã§.\\n======================================================\")\n",
    "    break;\n",
    "      \n",
    "result = [[0 for col in range(2)] for row in range(5)]\n",
    "    \n",
    "for i in range(5):\n",
    "    result[i][0] = generate_text_ballad(sequence,max_len)\n",
    "    result[i][1] = generate_text_hiphop(sequence,max_len)\n",
    "    \n",
    "print(\"\\nÎÖ∏Îûò Í∞ÄÏÇ¨ ÏÉùÏÑ±Ïù¥ ÏôÑÎ£åÎêòÏóàÏäµÎãàÎã§!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "7f6354e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(result)\n",
    "df.columns = ['Î∞úÎùºÎìú', 'ÌûôÌï©']\n",
    "pd.set_option('display.colheader_justify','right')\n",
    "pd.set_option('display.max_columns', None) # Î™®Îì† Ïó¥ Ï∂úÎ†•\n",
    "pd.set_option('display.max_colwidth',None) # Ïó¥Ïùò ÌëúÏãúÎêòÎäî Í∏ÄÏûêÏàò Ï†úÌïú Ìï¥Ï†ú"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "767b0cd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ÏÇ∞ÌÉÄÌÅ¥Î°úÏä§ÎãòÏùò Ïò§ÎäòÏùò Ïù¥ÏïºÍ∏∞Î•º ÎÖ∏ÎûòÎ°ú ÎßåÎì§ÏóàÏñ¥Ïöî ‚ô¨\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_7c352_ th {\n",
       "  text-align: center;\n",
       "}\n",
       "#T_7c352_row0_col0, #T_7c352_row0_col1, #T_7c352_row1_col0, #T_7c352_row1_col1, #T_7c352_row2_col0, #T_7c352_row2_col1, #T_7c352_row3_col0, #T_7c352_row3_col1, #T_7c352_row4_col0, #T_7c352_row4_col1 {\n",
       "  text-align: left;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_7c352_\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th class=\"col_heading level0 col0\" >Î∞úÎùºÎìú</th>\n",
       "      <th class=\"col_heading level0 col1\" >ÌûôÌï©</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_7c352_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_7c352_row0_col0\" class=\"data row0 col0\" >Î©îÎ¶¨ÌÅ¨Î¶¨Ïä§ÎßàÏä§, ÌôîÏù¥Ìä∏ ÌÅ¨Î¶¨Ïä§ÎßàÏä§ Ìä∏Î¶¨ Ï£ºÍ≥† Ïã∂Ïñ¥ Í∏¥Í∏¥ Î∞§ÏÉàÎèÑÎ°ù Ï≤úÏÇ¨ Í∞ôÏùÄ Í∑∏ÎåÄÏôÄ Îàà ÎßûÏ∂îÎ©∞ ÎÑå Ïó¨Í∏∞ÍπåÏßÄ ÎÑàÎßå ÏûàÍ∏∏ ÎßàÏùå Îã§Ìï¥ Í∏∞ÎèÑÌï¥ ÎÇ¥ ÎßòÏÜç Ìïú ÏÇ¨Îûå ÎÑ§Í∞Ä Î∞îÎ°ú ÎÑàÎùºÏÑú Í∑∏ÎûòÏÑú ÎÇòÎøêÏù¥Ïïº Îùº Îùº Îùº ÎùºÎùºÎùº ÎûÑÎùº Îùº ÎùºÎùºÎùºÎûÑÎùº ÎÇú ÎÑ§ ÏïàÏóê ÏûàÏñ¥ ÎùºÎùºÎùºÎûÑÎùº ÎÑàÎ•º Í∏∞Îã§Î†∏Ïñ¥ ÎÑàÎ¨¥ Ïò§Îûú Î∞§ ÏÉàÎ≤ΩÏùÑ Íπ®Ïö∞Îäî ÏûëÏùÄ ÏÜåÎÇòÍ∏∞Îäî Ïñ∏Ï†úÎÇò ÎÑàÎ•º Îñ†Ïò¨Î¶¨Í≥† ÏûàÏóàÏßÄ ÎÇ† Í∞êÏã∏ Ï§Ä ÎÑàÏïº ÏÇ¨ÎûëÏù¥ÏóàÎã®Í±∏ Îßê</td>\n",
       "      <td id=\"T_7c352_row0_col1\" class=\"data row0 col1\" >Î©îÎ¶¨ÌÅ¨Î¶¨Ïä§ÎßàÏä§, ÌÅ¨Î¶¨Ïä§ÎßàÏä§ Ï∫êÎ°§ Îçî Î∂àÎü¨Î¥ÖÎãàÎã§ Í∞ôÏù¥ Ï†úÎ∞ú call me nowhere so good lifestyle to be times i don t want your love and let me out for a foolish girl i wanna be with u baby some thing ÎßâÏì∞Í≥† Îã§Ïì∞Í≥† Ïì¥Í±∞Î•º Îã§ pay back Ïù¥Î†áÍ≤åÎùºÎèÑ Ìï¥Ïïº ÌñàÎÇò ÎÇú ÎèÑÎßùÍ∞ÄÏßÄ ÏïäÏïòÏúºÎ©¥ Ìï¥ Ïö∞Î¶¨ Ïû°Ìûò</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7c352_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_7c352_row1_col0\" class=\"data row1 col0\" >Î©îÎ¶¨ÌÅ¨Î¶¨Ïä§ÎßàÏä§, ÎÇ¥ÎßòÏÜç ÌïúÏÇ¨Îûå ÎÖ∏Îû† Îì£Îã§ Ïö∏Î®πÏù¥Î©∞ Îã¨Î†§ÏôÄ ÎÇú ÎÑê Í∏∞Îã§Î†§ Îã§Ïãú Ìïú Î≤à ÎÑàÎ•º Î∞îÎùºÎ≥∏Îã§ Ïò§Îûú Í∏∞Îã§Î¶ºÏù¥ ÏßÄÎÇòÍ∞ÄÍ≥† ÏùµÏàôÌïú ÎÉÑÏÉà Í∞ÄÎìùÌïú Ïù¥ Í±∞Î¶¨Ïóê ÏïÑÏßÅ ÎÇØÏÑ† Ïù¥Ïùò ÏõÉÏùåÎì§Ïù¥ Í∞ÄÎìùÌï¥ Í∑∏ Ïñ¥Îñ§ Í≥†ÎØºÎèÑ ÌûòÍ≤®ÏõåÌïòÏßÄ ÏïäÏïÑ ÏßÄÍ∏à Ïù¥ ÏàúÍ∞ÑÎßå Í∏∞Îã§Î†§Ï§ò i love you Ïò§ÎäòÎèÑ ÎÇú ÎÑàÏùò Ìíà ÏïàÏóêÏÑú Ïû†Îì§ Ïàò ÏûàÏùÑÍπå Í∏∞ÏÅú ÎßàÏùåÏúºÎ°ú i love you ÎÑ§Í≤å ÏÜçÏÇ≠Ïó¨Ï§ÑÍ≤å Î©ÄÎ¶¨ ÏûàÏñ¥ÎèÑ ÎäêÎÇÑ Ïàò ÏûàÎäî ÎÇòÎßåÏùò ÏÜêÍ∏∏ i love y</td>\n",
       "      <td id=\"T_7c352_row1_col1\" class=\"data row1 col1\" >Î©îÎ¶¨ÌÅ¨Î¶¨Ïä§ÎßàÏä§, i m up on a rockstar Î∞îÏΩîÎìúÍ∞ôÏù¥ Í∑úÏπôÏ†ÅÏù∏ cold like a lightlightlightlightlightlightlightlightlightrunkrr ÎÇ† Î≥¥Î©∞ ÌôúÏßù ÏõÉÏñ¥ Í≥ÑÏÜç Î∞òÎ≥µÌï¥ ÎÑê Í∏∞Îã§Î†§Ï£ºÎäî ÏπúÍµ¨Ïïº ÎÑàÏóêÍ≤å ÎØ∏Ïïà ÎÑàÎ¨¥ Îä¶Ïñ¥Î≤ÑÎ¶∞ Í≤É Í∞ôÏïÑ ÎÑàÎ¨¥ ÎßéÏùÄ ÏãúÍ∞ÑÎì§Ïù¥ ÏûàÏßÄÎßå ÎÇú Ìï≠ÏÉÅ ÎÇ¥ ÏòÜÏùÑ ÏßÄÎÇòÏ≥êÏôîÏúºÎãàÍπå Ï°∞Í∏àÎßå Ï∞∏Îã§Ï∞∏Îã§Ï∞∏Îã§Ï∞∏Îã§ Î∞òÎ≥µÌïòÎã§ÎÅùÎÇú Í±¥Í∞Ä Ïñ¥Ï†ØÎ∞§ ÎÇ¥Í∞Ä Î≥∏</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7c352_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_7c352_row2_col0\" class=\"data row2 col0\" >Î©îÎ¶¨ÌÅ¨Î¶¨Ïä§ÎßàÏä§, ÎÇ¥ ÎßòÏÜç Î™®Îëê stop love you ÎÑàÏóêÍ≤å Í≥†Î∞±Ìï†Îûò stop love you ÎÑàÏôÄ ÍøàÍø®Îçò ÏãúÍ∞Ñ Ïñ∏Ï†úÎÇò ÎÇú ÏòÅÏõêÌï† Í±∞Ïïº the villain it s a way holding is not starry Í∏∞ÎÇòÍ∏¥ Í≤®Ïö∏Î∞§ÏùÄ Ïú†ÎÇúÌûà Ìè¨Í∑ºÌïú Î∞îÎûåÏù¥Íµ¨ÎÇò ÎÑê ÎßåÎÇòÎ©¥ Îäò ÎÑ§Í≤åÎßå Ìï≠ÏÉÅ ÏõÉÍ≥§ Ìï¥ Ïù¥Ï†ú Ï°∞Í∏à ÏßÄÎÇòÎ©¥ ÎÇòÏïÑÏßàÍπå ÏÉùÍ∞ÅÌñàÎäîÎç∞ ÏòàÏÉÅÏùÑ Íπ®Í≥† ÎÑ§Í≤å ÏûÖ ÎßûÏ∂ò ÏàúÍ∞Ñ ÎÇòÎ•º Ìñ•Ìï¥ ÏôÄ ÌÅ¨Í≤å Îì§Î¶¨ÎÑ§ ÌïòÏñÄ Îàà</td>\n",
       "      <td id=\"T_7c352_row2_col1\" class=\"data row2 col1\" >Î©îÎ¶¨ÌÅ¨Î¶¨Ïä§ÎßàÏä§, sorry Î™®Îëê Ïù¥Í≤®ÎÇ¥Í≥† bitechÎäî Îçî Ïª§Ï†∏ Ï£ºÎ®∏ÎãàÏóê iphoneÏùÑ Ï±ÑÏõå cowayÏóê Ïñ¥ÏßàÎü¨ÏßÑ ÏàòÎèóÎ¨ºÏóê Î≤ÑÌã¥ Î¨º drinking mottoÏóê Îã¥ÏßÄ hey you everyday get it we re here with you here without you i ll change this time out pretty lie everything everyday get it we</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7c352_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_7c352_row3_col0\" class=\"data row3 col0\" >Î©îÎ¶¨ÌÅ¨Î¶¨Ïä§ÎßàÏä§, Ïä¨ÌîÑÏßÄÎßå ÌñâÎ≥µÌïú Ïù¥ ÏàúÍ∞Ñ Ìï®Íªò ÌïòÎãàÍπå ÍøàÍøîÏò® Í∑∏ Î™®Ïäµ Ïó¨Ï†ÑÌûà ÏòàÏÅòÎã§ ÎÇ¥ ÏÜêÏùÑ Ïû°Í≥† Í±∑Îäî Ïù¥ christmas ÍøàÎßå Í∞ôÏïÑ Ï†ÑÎ∂Ä Îã§ ÎÇ¥ ÏÑ∏ÏÉÅ Í∞ôÏïÑ ÎÇòÏóêÍ≤åÎäî Ïù¥ ÏÑ∏ÏÉÅ Î¨¥ÏóáÎ≥¥Îã§ ÏÜåÏ§ëÌïú Í≤É Í∞ôÏïÑ ÎÑå Ìïú Îïå ÍøàÏóêÏÑúÎèÑ ÏÇ¨ÎûëÏúºÎ°ú Ïº∞Îçò Í∑∏ Í∏∞ÏñµÎßå ÏïÑÏßÅÎèÑ ÏÑ†Î™ÖÌï¥ ÎÇòÎäî ÌñâÎ≥µÌñàÏóàÎçò Í∑∏ÎïåÎ°ú Îã§Ïãú ÎèåÏïÑÍ∞Ä ÎÑàÏùò Î™®Îì† Ï∂îÏñµÏù¥ Îã¥Í∏¥ Ïù¥ Í∏∏ÏùÑ Îî∞Îùº Í±∏ÏúºÎ©∞ ÌïòÎÇò Îëò Í≤®Ïö∏ ÏßÄÎÇò Î¥ÑÏù¥ Ïò§ÎìØÏù¥ ÎÇòÎèÑ ÍΩÉÏù¥ ÌîºÍ≤†ÏßÄ ÎãπÏã†ÏùÑ ÎãÆÏïÑÍ∞àÎûò Îçî Îî∞ÎúªÌï†Í≤åÏöî</td>\n",
       "      <td id=\"T_7c352_row3_col1\" class=\"data row3 col1\" >Î©îÎ¶¨ÌÅ¨Î¶¨Ïä§ÎßàÏä§, maybe is your love cuz i know but i won t never let s get our fam ÎÇ¥ ÏπúÍµ¨Îì§ÏùÄ Í≥ß ÏóÜÏñ¥Ï†∏ Î≤ÑÎ¶¥ shit you gotta high ÎÇòÎäî ÏïÑÏßÅÎèÑ slave i can do that my life Ïò§Îäò Î∞§ÏùÄ Ï†ÑÎ∂Ä Îã§ riskin pain Ïò§ÎäòÏùÄ Ïâ¨Ïõå we re just go into you yeah i am s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7c352_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_7c352_row4_col0\" class=\"data row4 col0\" >Î©îÎ¶¨ÌÅ¨Î¶¨Ïä§ÎßàÏä§, ÌÅ¨Î¶¨Ïä§ÎßàÏä§ ÏÑ†Î¨ºÌïòÎÇò Ï§ÄÎπÑÎ™ªÌñàÏßÄÎßå ÌïòÏñóÍ≤å ÎààÏù¥ ÎÇ¥Î¶¨Îçò Í∑∏ÎÇ† ÏÉùÍ∞ÅÌïòÎ©∞ ÏÉàÌïòÏñòÏßÑ ÏñºÍµ¥ ÌïòÏñÄ ÎààÍΩÉÏù¥ ÎÇ†Î¶¨Îäî Ïù¥ ÏàúÍ∞ÑÏù¥ Ïò¨Í≤ÉÎßå Í∞ôÏïÑ ÎÇò Ïò§ÎäòÎèÑ ÌïòÏñóÍ≤å ÎààÏù¥ ÎÇ¥Î¶¨Îçò Í∑∏ÎÇ† ÏÉùÍ∞ÅÌï¥ Îñ†Ïò¨Î¶¨Î©∞ ÏÇ¨ÎûëÌñàÏóàÎçò Í∑∏ÎÇ†Îì§ ÏÉùÍ∞ÅÌïòÎ©∞ Ïû† Î™ª ÎìúÎäî ÎÇòÏùò ÎßàÏùåÏùÑ ÏïàÏì∞ÎüΩÎã®ÎßêÏïº ÏïàÎÖï ÏïàÎÖïÏïÑ ÎÇò Ïò§ÎäòÎèÑ ÌïòÏñóÍ≤å ÎààÏù¥ ÎÇ¥Î¶¨Îçò Í∑∏ÎÇ† ÏÉùÍ∞ÅÌïòÎ©¥ Îñ†Ïò¨Î¶¨Î©∞ ÏÇ¨ÎûëÌñàÏóàÎçò Í∑∏ ÏãúÍ∞ÑÎì§ ÏÉùÍ∞ÅÌïòÎ©∞ Ïù¥Ï†úÏÑúÏïº ÎÇ¥ ÎßàÏùå Ï∞®Í∞ëÍ≤å ÏãùÏñ¥Í∞ÄÎäî ÎÇòÎ•º Î≥¥Î©∞ ÌïúÏóÜÎäî ÎØ∏ÏÜåÍ∞Ä Î≤à</td>\n",
       "      <td id=\"T_7c352_row4_col1\" class=\"data row4 col1\" >Î©îÎ¶¨ÌÅ¨Î¶¨Ïä§ÎßàÏä§, feel myself and pause out of the queeneration apart now young bass my time but we made for me like i know that booth i can never get surfficing for the next chance where can never let s play with me done had to be t</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x2b0cc8dcfa0>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(your_name+\"ÎãòÏùò Ïò§ÎäòÏùò Ïù¥ÏïºÍ∏∞Î•º ÎÖ∏ÎûòÎ°ú ÎßåÎì§ÏóàÏñ¥Ïöî ‚ô¨\")\n",
    "dfStyler = df.style.set_properties(**{'text-align': 'left'}) # ÌÖçÏä§Ìä∏ ÏôºÏ™Ω Ï†ïÎ†¨\n",
    "dfStyler.set_table_styles([dict(selector='th', props=[('text-align', 'center')])]) # label Ï†úÎ™© Í∞ÄÏö¥Îç∞ Ï†ïÎ†¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9235af5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
